---
title: "Endpoint (LLM Call) Tool"
description: "Configure and use the Endpoint (LLM Call) Tool to make external API calls to language models"
icon: 'grid-2x2-plus'
---

The Endpoint (LLM Call) Tool allows your assistant to make external API calls to language models when responding to user queries. This guide will walk you through the process of adding and configuring this tool for your assistant.

## Prerequisites

Before adding the Endpoint (LLM Call) Tool, ensure that you have:

- [Created an assistant](/assistants/create-assistant)
- [Created an endpoint](/endpoint/create-endpoint)
- Access to an external LLM API endpoint

## Adding the Endpoint (LLM Call) Tool

<Steps>
  <Step title="Navigate to Your Assistant">
    1. Go to the Assistants section in the main navigation menu.
    2. Select the assistant you want to configure.
    3. Navigate to the "Manage" -> "Tools" tab.
    4. Click on **Add Tools**.
  </Step>

  <Step title="Select the Endpoint (LLM Call) Tool">

  ![Navigate to Assistants](/images/assistant/tools/endpoint-tool.png)
    1. From the list of available tools, find "Endpoint (LLM Call)".
    2. Click on the **Add Tool** button next to it.
  </Step>

  <Step title="Configure the Tool">
    Once added, you can configure the Endpoint (LLM Call) Tool with the following settings:
    
    <AccordionGroup>
    <Accordion title="Tool Name">
      Endpoint (LLM Call)
    </Accordion>

    <Accordion title="Description">
      Use this tool to make external API calls to language models for additional processing or specialized tasks.
    </Accordion>

    <Accordion title="Fields">
      <Accordion title="endpoint">
        - **Type**: String
        - **Description**: The URL of the external LLM API endpoint
        - **Required**: Yes
      </Accordion>

      <Accordion title="prompt">
        - **Type**: String
        - **Description**: The prompt or query to send to the external LLM
        - **Required**: Yes
      </Accordion>

      <Accordion title="parameters">
        - **Type**: Object
        - **Description**: Additional parameters for the API call (e.g., temperature, max_tokens)
        - **Required**: No
      </Accordion>
    </Accordion>

    <Accordion title="When to Use">
      The assistant should use this tool when:
      - Specialized language processing is required
      - The task benefits from an external LLM's capabilities
      - Additional context or processing is needed beyond the assistant's built-in abilities
    </Accordion>
    </AccordionGroup>
  </Step>

  <Step title="Save Configuration">
    After configuring the tool, click on the **Save** or **Update** button to apply your changes.
  </Step>
</Steps>

## Using the Endpoint (LLM Call) Tool

Once configured, your assistant can use the Endpoint (LLM Call) Tool when appropriate. The process typically follows these steps:

1. The assistant determines a need for external LLM processing.
2. It calls the Endpoint (LLM Call) Tool with the necessary parameters.
3. The tool makes an API call to the specified endpoint.
4. The assistant incorporates the response from the external LLM into its own response.

## Managing the Endpoint (LLM Call) Tool

You can modify or remove the Endpoint (LLM Call) Tool at any time:

1. Go to your assistant's "Manage" -> "Tools" tab.
2. Find the Endpoint (LLM Call) Tool in the list.
3. Click on **Edit** to modify its configuration or **Remove** to delete it.

## Best Practices

- Ensure the external LLM API endpoint is reliable and secure.
- Monitor API usage and costs associated with external calls.
- Use the tool judiciously to balance between built-in capabilities and external processing.
- Regularly review and update the tool's configuration to optimize performance.

By effectively configuring and utilizing the Endpoint (LLM Call) Tool, your assistant can leverage external language models for enhanced capabilities and specialized processing tasks.